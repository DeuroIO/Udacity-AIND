{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bookworm\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this project, you will build a simple question-answering agent that is able to learn from any text data you provide, and answer queries posed in natural language. You will use IBM Watson's cloud-based services to process the input text data and find relevant responses.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this project, you will learn how to:\n",
    "\n",
    "- Create a cloud-based NLP service instance and configure it.\n",
    "- Ingest a set of text documents using the service and analyze the results.\n",
    "- Accept questions in natural language and parse them.\n",
    "- Find relevant answers from the preprocessed text data.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In order to use Watson's cloud-based services, you first need to create an account on the [IBM Bluemix platform](https://console.ng.bluemix.net/).\n",
    "\n",
    "<div>\n",
    "    <div style=\"display: table-cell; width: 50%;\">\n",
    "        <img src=\"images/watson-logo.png\" alt=\"IBM Watson logo\" width=\"200\" />\n",
    "    </div>\n",
    "    <div style=\"display: table-cell; width: 50%;\">\n",
    "        <img src=\"images/bluemix-logo.png\" alt=\"IBM Bluemix logo\" width=\"400\" />\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "Then, for each service you want to use, you have to create an instance of that service. You can continue with the tasks below, and create a service instance when indicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create and configure Discovery service\n",
    "\n",
    "Create an instance of the **Discovery** service. You will use this to process a set of text documents, and _discover_ relevant facts and relationships.\n",
    "\n",
    "- Go to the [IBM Bluemix Catalog](https://console.ng.bluemix.net/catalog/?taxonomyNavigation=services&category=watson).\n",
    "- Select the service you want, **Discovery**, under the **Watson** category.\n",
    "- Enter a Service Name for that instance, e.g. **Disco1** and a Credential Name, e.g. **Disco1-Creds** (these are just for you to be able to refer to later, they do not affect the functioning of the service).\n",
    "- You should be able to see your newly-created service in your [Services Dashboard](https://console.ng.bluemix.net/dashboard/services).\n",
    "- Open the service instance, click on the **Service credentials** tab, and then **View credentials** under Actions. This is where you will find the username and password to use when connecting to the service.\n",
    "\n",
    "<img src=\"images/discovery-creds.png\" alt=\"Discovery Service - Credentials tab\" width=\"800\" />\n",
    "\n",
    "Save the credentials for the discovery service in a JSON file in the current directory named `service-credentials.json` with the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"discovery\": {\n",
    "        \"username\": \"<your Discovery username here>\",\n",
    "        \"password\": \"<your Discovery password here>\"\n",
    "    },\n",
    "    \"conversation\": {\n",
    "        \"username\": \"\",\n",
    "        \"password\": \"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "You will be filling out the Conversation service credentials later, when you create an instance for it. Note that you should keep these credentials secret. Please do not turn them in with your submission!\n",
    "\n",
    "### Connect to the service instance\n",
    "\n",
    "Let's connect to the service instance you just created using IBM Watson's [Python SDK](https://github.com/watson-developer-cloud/python-sdk). You will first need to install the SDK:\n",
    "```bash\n",
    "pip install watson-developer-cloud\n",
    "```\n",
    "\n",
    "Now execute each code cell below using **`Shift+Enter`**, and complete any steps indicated by a **`TODO`** comment. For more information on the Discovery service, please read the [Documentation](https://www.ibm.com/watson/developercloud/doc/discovery/index.html) and look at the [API Reference](https://www.ibm.com/watson/developercloud/discovery/api/v1/?python) as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Usual Python imports\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# BeautifulSoup, for parsing HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Matplotlib, for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Watson Python SDK\n",
    "import watson_developer_cloud\n",
    "\n",
    "# Utility functions\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Connect to the Discovery service instance\n",
    "# TODO: Ensure that your username and password from the Service Credentials tab are in service-credentials.json\n",
    "# Note that these credentials are different from your IBM Bluemix login, and are specific to the service instance\n",
    "discovery_creds = helper.fetch_credentials('discovery')\n",
    "discovery = watson_developer_cloud.DiscoveryV1(\n",
    "    version='2016-11-07',\n",
    "    username=discovery_creds['username'],\n",
    "    password=discovery_creds['password'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment\n",
    "\n",
    "The Discovery service organizes everything needed for a particular application in an _environment_. Let's create one called \"Bookworm\" for this project.\n",
    "\n",
    "> _**Note**: It is okay to run this block multiple times - it will not create duplicate environments with the same name._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare an environment to work in\n",
    "env, env_id = helper.fetch_object(\n",
    "    discovery, \"environment\", \"Bookworm\",\n",
    "    create=True, create_args=dict(\n",
    "        description=\"A space to read and understand stories\",  # feel free to edit\n",
    "        size=0  # use 0 for free plan (see API reference for more on sizing)\n",
    "    ))\n",
    "print(json.dumps(env, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify configuration options\n",
    "\n",
    "A _configuration_ defines what natural language processing routines are applied to any documents that are submitted to the service. Each environment gets a default configuration when it is created.\n",
    "\n",
    "You can fetch the default configuration and view the different options using the following piece of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View default configuration\n",
    "cfg_id = discovery.get_default_configuration_id(environment_id=env_id)\n",
    "cfg = discovery.get_configuration(environment_id=env_id, configuration_id=cfg_id)\n",
    "print(json.dumps(cfg, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 main configuration blocks that affect how input documents are processed:\n",
    " \n",
    "1. **`conversions`**: How to convert documents in various formats (Word, PDF, HTML) and extract elements that indicate some structure (e.g. headings).\n",
    "2. **`enrichments`**: What NLP output results are we interested in (keywords, entities, sentiment, etc.).\n",
    "3. **`normalizations`**: Post-processing steps to be applied to the output. This can be left empty in most cases, unless you need the output to be normalized into a very specific format.\n",
    "\n",
    "_**Note**: The default configuration for an environment cannot be modified. If you need to change any of the options, you will need to create a new one, and then edit it. The easiest way to do this is using the service dashboard, which is described later._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your configuration\n",
    "\n",
    "It is a good idea to test your configuration on a small sample text before you apply it to a larger document collection.\n",
    "\n",
    "_**Note**: We have supplied a sample document (`data/sample.html`) containing the opening crawl text for Star Wars: Episode IV, but you are free to use a text of your choosing._\n",
    "\n",
    "**Q**: (optional) If you use your own sample text, provide a brief title and description below.\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test configuration on some sample text\n",
    "data_dir = \"data\"\n",
    "filename = os.path.join(data_dir, \"sample.html\")\n",
    "with open(filename, \"r\") as f:\n",
    "    res = discovery.test_document(environment_id=env_id, configuration_id=cfg_id, fileinfo=f)\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze test output\n",
    "\n",
    "The results returned by the service contain a _snapshot_ of the information extracted at each step of processing - document conversions, enrichments and normalizations. We are interested in the output of applying enrichments (`\"enrichments_output\"`) or after normalizing them (`\"normalizations_output\"`). These should be identical if no post-processing/normalizations were specified in the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a closer look at the results from the \"enrichments_output\" or \"normalizations_output\" step\n",
    "output = next((s[\"snapshot\"] for s in res[\"snapshots\"] if s[\"step\"] == \"enrichments_output\"), None)\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions based on the output above. Note that it contains the input HTML, extracted text and metadata as well as the actual enrichment results (`\"enriched_text\"` block).\n",
    "\n",
    "#### Sentiment\n",
    "\n",
    "**Q**: What is the overall sentiment detected in this text? Mention the `type` (positive/negative) and `score`.<br />\n",
    "(_Hint: Look for the `\"docSentiment\"` key in the output._)\n",
    "\n",
    "**A**: \n",
    "\n",
    "\n",
    "#### Concepts\n",
    "\n",
    "**Q**: List 3 concepts that have been identified with a relevance > 0.5. Note that not all concepts here may be present directly in the text, some may have been inferred by Watson.<br />\n",
    "(_Hint: Look for `\"concepts\"`._)\n",
    "\n",
    "**A**:\n",
    "\n",
    "\n",
    "#### Relations\n",
    "\n",
    "Each relation is essentially a deeper analysis of a sentence (or part of a sentence). Here is a sample relation:\n",
    "```json\n",
    "{\n",
    "  \"sentence\": \" During the battle, Rebel spies managed to steal secret plans to the Empire's ultimate weapon, the DEATH STAR, an armored space station with enough power to destroy an entire planet.\",\n",
    "  \"subject\": {\n",
    "    \"text\": \"Rebel spies\",\n",
    "    \"keywords\": [\n",
    "      {\n",
    "        \"text\": \"Rebel spies\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"action\": {\n",
    "    \"text\": \"managed to steal\",\n",
    "    \"lemmatized\": \"manage to steal\",\n",
    "    \"verb\": {\n",
    "      \"text\": \"steal\",\n",
    "      \"tense\": \"future\"\n",
    "    }\n",
    "  },\n",
    "  \"object\": {\n",
    "    \"text\": \"secret plans\",\n",
    "    \"keywords\": [\n",
    "      {\n",
    "        \"text\": \"secret plans\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    " }\n",
    "```\n",
    "\n",
    "In this case, Watson seems to have done a pretty good job of extracting some meaning from the sentence.\n",
    "\n",
    "**Q**: Find a relation where the extracted meaning is not as accurate, or not what you would've expected. List the `sentence`, `subject`, `action` and `object` parts as identified, and what you would've marked instead.<br />\n",
    "(_Hint: Look for `\"relations\"`._)\n",
    "\n",
    "**A**: \n",
    "\n",
    "#### Keywords\n",
    "\n",
    "You may have noticed that Watson identifies some `\"keywords\"` in the relations, e.g. `\"Rebel spies\"` and `\"secret plans\"` in the Star Wars example above. The output also contains a list of all keywords at the top level, for your convenience, along with their relevance to the document and sentiment conveyed. Let's visualize these keywords as a word cloud!\n",
    "\n",
    "Note: We'll be using this handy [worldcoud library](https://github.com/amueller/word_cloud) to generate the visualization. So you will need to install it first:\n",
    "```bash\n",
    "pip install wordcloud\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize keywords by relevance as a wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wc_data = { w[\"text\"]: w[\"relevance\"] for w in output[\"enriched_text\"][\"keywords\"] }\n",
    "wc = WordCloud(width=400, height=300, scale=2, background_color=\"white\", colormap=\"Vega10\")\n",
    "wc.generate_from_frequencies(wc_data)  # use precomputed relevance instead of frequencies\n",
    "\n",
    "plt.figure(figsize=(4, 3), dpi=200)\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play with this visualization and improve it. What about using a different metric instead of relevance, e.g. direct word frequencies that the wordcloud library computes by default?\n",
    "\n",
    "#### Other results\n",
    "\n",
    "Watson's output also includes processed results from other enrichments that were applied to the text, including entities and taxonomy (what topic or category does this text relate to).\n",
    "\n",
    "```json\n",
    "\"taxonomy\": [\n",
    "  {\n",
    "    \"label\": \"/art and entertainment/movies and tv/movies\",\n",
    "    \"score\": 0.584247,\n",
    "    \"confident\": false\n",
    "  },\n",
    "  {\n",
    "    \"label\": \"/society/unrest and war\",\n",
    "    \"score\": 0.517031,\n",
    "    \"confident\": false\n",
    "  },\n",
    "  {\n",
    "    \"confident\": false,\n",
    "    \"label\": \"/law, govt and politics/armed forces/army\",\n",
    "    \"score\": 0.215561\n",
    "  }\n",
    "],\n",
    "```\n",
    "\n",
    "Get a good sense of all the different pieces of information available in the results. Start thinking about which ones will be useful for looking up answers to questions, and how you might use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ingest documents\n",
    "\n",
    "### Create a collection\n",
    "\n",
    "A _collection_ is used to organize documents of the same kind. For instance, you may want to create a collection of book reviews, or a collection of Wikipedia articles, but it may not make much sense to mix the two groups. This allows Watson to make meaningful inferences over the set of documents, find commonalities and identify important concepts.\n",
    "\n",
    "Let's create one called \"Story Chunks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare a collection of documents to use\n",
    "col, col_id = helper.fetch_object(discovery, \"collection\", \"Story Chunks\", environment_id=env_id,\n",
    "    create=True, create_args=dict(\n",
    "        environment_id=env_id, configuration_id=cfg_id,\n",
    "        description=\"Stories and plots split up into chunks suitable for answering\"\n",
    "    ))\n",
    "print(json.dumps(col, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created a collection, you should be able to view it using the Discovery Service tool. To open, go to the **Manage** tab for your service instance, and click the **Launch tool** button.\n",
    "\n",
    "<img src=\"images/discovery-manage.png\" alt=\"Discovery service - Manage tab\" width=\"800\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should see the \"Story Chunks\" collection you just created.\n",
    "\n",
    "<img src=\"images/discovery-tooling.png\" alt=\"Discovery service - Tool showing collections\" width=\"800\" />\n",
    "\n",
    "You can open the collection to view more details about it. If you need to modify configuration options, click the **Switch** link and create a new configuration (the default one cannot be changed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add documents\n",
    "\n",
    "Okay, now that we have everything set up, let's add a set of \"documents\" we want Watson to look up answers from, using the Python SDK. Note that Watson treats each \"document\" as a unit of text that is returned as the result of a query. But we want to retrieve a paragraph of text for each question. So, let's split each file up into individual paragraphs. We will use the [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) library for this purpose.\n",
    "\n",
    "_**Note**: You could also add and manage documents in the collection using the Discovery tool, but you would have to split paragraphs up into separate files._\n",
    "\n",
    "_**Note**: We have provided a set of files (`data/Star-Wars/*.html`) with summary plots for Star Wars movies, but you are free to use a collection of your choice. Open one of the files in a text editor to see how the paragraphs are delimited using `<p>...</p>` tags - this is how the code block below split paragraphs into separate \"documents\"._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add documents to collection\n",
    "doc_ids = []  # to store the generated id for each document added\n",
    "for filename in glob.glob(os.path.join(data_dir, \"Star-Wars\", \"*.html\")):\n",
    "    print(\"Adding file:\", filename)\n",
    "    with open(filename, \"r\") as f:\n",
    "        # Split each individual <p> into its own \"document\"\n",
    "        doc = f.read()\n",
    "        soup = BeautifulSoup(doc, 'html.parser')\n",
    "        for i, p in enumerate(soup.find_all('p')):\n",
    "            doc_info = discovery.add_document(environment_id=env_id, collection_id=col_id,\n",
    "                file_data=json.dumps({\"text\": p.get_text(strip=True)}),\n",
    "                mime_type=\"application/json\",\n",
    "                metadata={\"title\": soup.title.get_text(strip=True)})\n",
    "            doc_ids.append(doc_info[\"document_id\"])\n",
    "print(\"Total\", len(doc_ids), \"documents added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the collection details, you may notice that the `\"document_counts\"` field now shows some documents as `available` or `processing`. Once processing is complete, you should see all the documents under the `available` count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View collection details to verify all documents have been processed\n",
    "col, col_id = helper.fetch_object(discovery, \"collection\", \"Story Chunks\", environment_id=env_id)\n",
    "print(json.dumps(col, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what did the Discovery service learn? If you list the fields extracted from the set of documents in the collection as part of the enrichment process, you'll see familiar fields like `concepts`, `entities` and `keywords` that were returned in the test analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List all fields extracted\n",
    "discovery.list_collection_fields(environment_id=env_id, collection_id=col_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test query\n",
    "\n",
    "Let's perform a simple query to see if the service can fetch the proper document for us:\n",
    "> _Look for all paragraphs that have a relation (sentence) with \"Jar Jar\" as the subject, and return the title and text._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A simple query\n",
    "results = discovery.query(environment_id=env_id, collection_id=col_id,\n",
    "    query_options={\n",
    "        \"query\": \"enriched_text.relations.subject.text:\\\"Jar Jar\\\"\",\n",
    "        \"return\": \"metadata.title,text\"\n",
    "    })\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the above query and see what results you get! Try to find one that returns relevant results, and keep that (along with the output) for review.\n",
    "\n",
    "> See [Query building reference](https://www.ibm.com/watson/developercloud/doc/discovery/query-reference.html) for descriptions of all possible parameters, operators and aggregations. You can also choose to build the query using the web interface (click the \"Story Chunks\" collection to query it), and then reproduce the query here.\n",
    "\n",
    "Then answer the questions below:\n",
    "\n",
    "**Q**: What query did you try? Express it in plain words below.\n",
    "\n",
    "**A**:\n",
    "\n",
    "\n",
    "**Q**: What answer did you get back from Watson? You only need to mention the relevant snippet of text fro mthe paragraph(s) returned.\n",
    "\n",
    "**A**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parse natural language questions\n",
    "\n",
    "In order to understand questions posed in natural language, we'll use another Watson service called [Conversation](https://www.ibm.com/watson/developercloud/doc/conversation/index.html). It can be used to design conversational agents or _chatbots_ that exhibit complex behavior, but for the purpose of this project, we'll only use it to parse certain kinds of queries.\n",
    "\n",
    "### Create a Conversation service instance\n",
    "\n",
    "Just like you did for the Discovery service, create an instance of the Conversation service. Then launch the associated tool from the service dashboard.\n",
    "\n",
    "<img src=\"images/conversation-tooling.png\" alt=\"Conversation service - Tool homepage\" width=\"800\" />\n",
    "\n",
    "A _workspace_ allows you to keep all the items you need for a particular application in one place, just like an _environment_ in case of the Discovery service. Create one called \"Bookworm\" with a suitable description, such as \"I know a lot of stories. Ask me a question!\"\n",
    "\n",
    "<img src=\"images/conversation-workspace.png\" alt=\"Conversation service - Blank workspace\" width=\"800\" />\n",
    "\n",
    "This should open up a blank workspace, where you can add intents, define the entities you want the agent to idenitfy and structure the overall dialog.\n",
    "\n",
    "### Add intents\n",
    "\n",
    "An _intent_ is the goal or purpose of a user's input. Create a set of intents (at least 3) that capture the different kinds of questions that you want the system to answer, e.g. _who_, _what_ and _where_. Along with each intent, add a list of user examples or _utterances_ that map to that intent.\n",
    "\n",
    "For instance, you could enter the following examples for the _where_ intent:\n",
    "\n",
    "- Where is the Jedi temple located?\n",
    "- Where was Luke born?\n",
    "\n",
    "The Conversation service recommends at least 5 examples for each intent so that Watson learns how to recognize it. These don't have to be very precise, but more examples the better.\n",
    "\n",
    "<img src=\"images/conversation-intents.png\" alt=\"Conversation service - Intents listed\" width=\"800\" />\n",
    "\n",
    "Feel free to create your own intents, based on the kinds of questions you want the system to answer, e.g. \"How many ...\", \"What are the most common ...\" etc. Each intent will need to be mapped to an appropriate query.\n",
    "\n",
    "> See [**Defining intents**](https://www.ibm.com/watson/developercloud/doc/conversation/intents.html) for a helpful video and further instructions.\n",
    "\n",
    "**Q**: What intents did you add to the Conversation service instance?\n",
    "\n",
    "**A**:\n",
    "\n",
    "\n",
    "\n",
    "**Q**: Pick one of these intents, and list at least 5 examples for the intent that you entered.\n",
    "\n",
    "**A**:\n",
    "\n",
    "\n",
    "\n",
    "### Add entities\n",
    "\n",
    "Once you have your intents set, let's tell the service what entities we want it to identify. One way to do this is using the tool interface, and entering them one-by-one.\n",
    "\n",
    "> Go to [**Defining entities**](https://www.ibm.com/watson/developercloud/doc/conversation/entities.html) to see how that is done.\n",
    "\n",
    "But that can be tedious! So let's refer back to the entities that the Discovery service identified, and load them in programmatically.\n",
    "\n",
    "As before, let's connect to the Conversation service first. Remember to enter your service credentials below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to the Conversation service instance\n",
    "# TODO: Enter your username and password from the Service Credentials tab in service-credentials.json\n",
    "conversation_creds = helper.fetch_credentials('conversation')\n",
    "conversation = watson_developer_cloud.ConversationV1(\n",
    "    version='2017-02-03',\n",
    "    username=conversation_creds['username'],\n",
    "    password=conversation_creds['password'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the workspace you just created called \"Bookworm\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wrk, wrk_id = helper.fetch_object(conversation, \"workspace\", \"Bookworm\")\n",
    "print(json.dumps(wrk, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all the entities from the Discovery service collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all the entities from the collection and group them by type\n",
    "response = discovery.query(environment_id=env_id, collection_id=col_id,\n",
    "    query_options={\n",
    "        \"return\": \"enriched_text.entities.type,enriched_text.entities.text\"\n",
    "    })\n",
    "\n",
    "# Group individual entities by type (\"Person\", \"Location\", etc.)\n",
    "entities_by_type = {}\n",
    "for document in response[\"results\"]:\n",
    "    for entity in document[\"enriched_text\"][\"entities\"]:\n",
    "        if entity[\"type\"] not in entities_by_type:\n",
    "            entities_by_type[entity[\"type\"]] = set()\n",
    "        entities_by_type[entity[\"type\"]].add(entity[\"text\"])\n",
    "\n",
    "# Ignore case to avoid duplicates\n",
    "for entity_type in entities_by_type:\n",
    "    entities_by_type[entity_type] = {\n",
    "        e.lower(): e for e in entities_by_type[entity_type]\n",
    "    }.values()\n",
    "\n",
    "# Restructure for loading into Conversation workspace\n",
    "entities_grouped = [{\n",
    "    \"entity\": entity_type,\n",
    "    \"values\": [{\"value\": entity} for entity in entities]}\n",
    "        for entity_type, entities in entities_by_type.items()]\n",
    "entities_grouped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the workspace with these entities and verify that have been added correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add these grouped entities to the Conversation workspace\n",
    "conversation.update_workspace(workspace_id=wrk_id, entities=entities_grouped)\n",
    "\n",
    "workspace_details = conversation.get_workspace(workspace_id=wrk_id, export=True)\n",
    "print(json.dumps(workspace_details[\"entities\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Note**: Ensure that at least 3 entity types, with at least 1 example entity each have been added._\n",
    "\n",
    "Here is what the list of entities should look like through the Conversation tool.\n",
    "\n",
    "<img src=\"images/conversation-entities.png\" alt=\"Conversation service - Entities listed\" width=\"800\" />\n",
    "\n",
    "**Q**: Name 3 entity types that were added, with at least 1 example entity each (e.g. entity type: _City_, example: _Los Angeles_).\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design dialog flow\n",
    "\n",
    "As a final step in creating the Conversation interface, let's design a typical dialog with a user. The most intuitive way to do this is to use the Dialog tab in the tool. Here, you can add _nodes_ that capture different stages in the dialog flow, and connect them in a meaningful way.\n",
    "\n",
    "Go ahead and add at least 3 dialog nodes. Specify the triggers in terms of the intents and entities that you'd like to match, and an optional intermediate response like \"Let me find that out for you.\" The actual response will be fetched by querying the Discovery service.\n",
    "\n",
    "Here is what the dialog nodes should look like.\n",
    "\n",
    "<img src=\"images/conversation-dialog_nodes.png\" alt=\"Conversation service - Dialog nodes\" width=\"640\" />\n",
    "\n",
    "**Q**: Specify 3 dialog nodes you added, along with the trigger (intent and/or entities) for each.\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dialog\n",
    "\n",
    "Let's run through a test dialog to demonstrate how the system transitions to one of the nodes you defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing the dialog flow\n",
    "\n",
    "# Start conversation with a blank message\n",
    "results = conversation.message(workspace_id=wrk_id, message_input={})\n",
    "context = results[\"context\"]\n",
    "\n",
    "# Then ask a sample question\n",
    "question= \"Who is Luke's father?\"\n",
    "results = conversation.message(workspace_id=wrk_id, message_input={\n",
    "    \"text\": question,\n",
    "    \"context\": context\n",
    "})\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query document collection to fetch answers\n",
    "\n",
    "The Discovery service includes a simple mechanism to make queries against your enriched collection of documents. But you have a lot of control over what fields are searched, how results are aggregated and values are returned.\n",
    "\n",
    "### Process sample question\n",
    "\n",
    "Choose a sample nautal language question to ask, and run it through the Conversation service, just like you did above when testing dialog flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Run a sample question through Conversation service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extract the intent and entities identified in the question, and optionally what dialog node was triggered (in case you need it later to customize your response). Some sample code is provided below, but you may need to modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Identify the intent(s) the user expressed (typically a single one)\n",
    "query_intents = [intent[\"intent\"] for intent in results[\"intents\"]]\n",
    "print(\"Intent(s):\", query_intents)\n",
    "\n",
    "# TODO: Extract the entities found in the question text\n",
    "query_entities = [entity[\"value\"] for entity in results[\"entities\"]]\n",
    "print(\"Entities:\", query_entities)\n",
    "\n",
    "# TODO: (optional) Find out what dialog node was triggered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the collection\n",
    "\n",
    "Design a query based on the information extracted above, and run it against the document collection. The sample query provided below simple looks for all the entities in the raw `text` field. Modify it to suit your needs.\n",
    "\n",
    "Take a look at the [API Reference](https://www.ibm.com/watson/developercloud/discovery/api/v1/?python#query-collection) to learn more about the query options available, and for more guidance see this [documentation page](https://www.ibm.com/watson/developercloud/doc/discovery/using.html).\n",
    "\n",
    "_**Note**: You may want to design different queries based on the intent / dialog node that was triggered._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Query the Discovery service based on the intent and entities\n",
    "query_results = discovery.query(environment_id=env_id, collection_id=col_id,\n",
    "    query_options={\n",
    "        \"query\": \"text:{}\".format(\",\".join(\"\\\"{}\\\"\".format(e) for e in query_entities)),\n",
    "        \"return\": \"text\"\n",
    "    })\n",
    "print(json.dumps(query_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process returned results\n",
    "\n",
    "If you properly structure the query, Watson is able to do a pretty good job of finding the relevant information. But the result returned is a JSON object. Now your task is to convert that result into an appropriate response that best addresses the original natural language question that was asked.\n",
    "\n",
    "E.g. if the question was \"Who saved Han Solo from Jabba the Hutt?\" the answer should ideally just be \"The Rebels\" and not the entire paragraph describing Han Solo's rescue. But that can be a backup response if you cannot be more specific.\n",
    "\n",
    "_**Note**: You may have to go back to the previous step and modify the query, especially what you want the Discovery service to return, and this may depend on the intent / dialog node triggered. E.g. study the different parts of a \"relation\" structure to see how you might construct queries to match them._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Process returned results and compose an appropriate response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reflections\n",
    "\n",
    "**Q**: Now that you have gone through this exercise of designing a system that uses two IBM Watson services, what did you learn? What were some of the strengths and weaknesses of this approach?\n",
    "\n",
    "**A**:\n",
    "\n",
    "\n",
    "## (Optional) Extensions\n",
    "\n",
    "We have provided a set of sample data files containing Star Wars plot summaries. But as mentioned before, you are free to use your own dataset. In fact, a larger dataset maybe more suitable for use with IBM Watson's NLP services. If you used your own dataset, answer the following questions.\n",
    "\n",
    "**Q**: What dataset did you use, and in what ways is it different from the sample files provided?\n",
    "\n",
    "**A**:\n",
    "\n",
    "\n",
    "**Q**: Either include your dataset in the .zip file or repository you submit, or provide clear instructions on how to obtain the dataset, so that your reviewer can run your notebook or inspect the data to verify your results.\n",
    "\n",
    "**A**: \n",
    "\n",
    "\n",
    "_You can also design a web-based application that utilizes these services and deploy that on Bluemix! If you do, please share with your instructors and peers._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
